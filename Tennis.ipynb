{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install dependencies\n",
    "Most importantly install [Unity ML-agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md), PyTorch, and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the Unity enviroment downloaded and change the path of the file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **brains** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exame the State and Action Spaces\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 3 stacked instances of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate and initialize the agent\n",
    "The learning agent is imported from a separate file \"./agent.py\" and takes `state_size`, `action_size` and a `seed` as instance variables.\n",
    "\n",
    "A few highlights of the agents:\n",
    "- The agents select the policy given by the actor-critic network\n",
    "- The agents use a shared buffer to store recent steps `(state, action, reward, next_state, done)` tuples and replay them\n",
    "- The agents maximize reward based on an actor-critic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from buffer import ReplayBuffer\n",
    "from maddpg import MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup configuration                \n",
    "config = Config()\n",
    "\n",
    "# general config\n",
    "config.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.seed = 0\n",
    "\n",
    "# environment related config\n",
    "config.state_size = env_info.vector_observations.shape[1]    # size of the observation space (state space)\n",
    "config.action_size = brain.vector_action_space_size          # size of the action space\n",
    "config.num_agents = len(env_info.agents)                     # number of agents\n",
    "\n",
    "# Experience replay memory related config\n",
    "config.buffer_size = int(1e6)                                # size of the memory buffer\n",
    "config.batch_size = 256                                      # sample minibatch size\n",
    "config.memory = lambda: ReplayBuffer(config.action_size, config.buffer_size, config.batch_size, config.seed, config.device)\n",
    "\n",
    "# agent related info\n",
    "config.gamma = 0.99                                          # discount rate for future rewards\n",
    "config.tau = 0.02                                            # interpolation factor for soft update of target network\n",
    "config.lr_actor = 1e-4                                       # learning rate of Actor\n",
    "config.lr_critic = 3e-4                                      # learning rate of Critic\n",
    "config.weight_decay = 0                                      # L2 weight decay\n",
    "config.update_every = 4                                      # update every 20 time steps\n",
    "config.num_updates = 1                                       # number of updates to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maddpg = MADDPG(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test randomly selected actions (untrained agents)\n",
    "Run randomly selected actions in the environment to see what happens to the score. This is similar to an **untrained** agents.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step. A window should pop up that allows you to observe the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train multiple agents with Deep Determinitic Policy Gradient (DDPG)\n",
    "The agent actually runs on an underlying actor-critic network. This is beneficial instead of using an typical deep Q-learning network (DQN) not only the environment's state space is large at 24 variables but the action space contains 2 continuous action variables. \n",
    "\n",
    "The setup is similar to the DQN with a local and target network; however, now there are separate networks to evaluate: the **actor** network for learning the optimal policy and the **critic** network for evaluating the selected action. \n",
    "\n",
    "Let's train the agents until they achieve an average of the maximum score of +0.5 over 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_multi(n_episodes=3000, max_t=1000):\n",
    "    '''\n",
    "    -------------------------------------------\n",
    "    Parameters\n",
    "    \n",
    "    n_episodes: # of episodes that the agent is training for\n",
    "    max_t:      # of time steps (max) the agent is taking per episode\n",
    "    -------------------------------------------\n",
    "    '''\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    max_score = -np.inf\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]             # turn on train mode of the environment\n",
    "        states = env_info.vector_observations                         # get the current state for each agent\n",
    "        maddpg.reset()                                                # reset the OU noise parameter \n",
    "        ep_scores = np.zeros(num_agents)                              # initialize the score for each agent\n",
    "        for t in range(max_t):\n",
    "            actions = maddpg.act(states)                              # select an action for each agent \n",
    "            env_info = env.step(actions)[brain_name]                  # send all actions to the environment\n",
    "            next_states = env_info.vector_observations                # get next state for each agent\n",
    "            rewards = env_info.rewards                                # get reward for each agent\n",
    "            dones = env_info.local_done                               # check if episode finished\n",
    "            maddpg.step(states, actions, rewards, next_states, dones) # agents record enviroment response in recent step\n",
    "            states = next_states                                      # set the state as the next state for the following step for each agent\n",
    "            ep_scores += rewards                                      # update the total score\n",
    "            if np.any(dones):                                         # exit loop if episode for any agent finished\n",
    "                break \n",
    "                \n",
    "        scores_deque.append(np.max(ep_scores))\n",
    "        scores.append(ep_scores)\n",
    "        \n",
    "        # print average epsiode score and average 100-episode score for each episode\n",
    "        print('\\rEpisode {} \\tMax Score: {:.2f} \\tAverage Max Score: {:.2f}'.format(i_episode, np.max(ep_scores), np.mean(scores_deque)), end=\"\")  \n",
    "        \n",
    "        # print and save actor and critic weights when a score of +30 over 100 episodes has been achieved\n",
    "        if np.mean(scores_deque) >= 0.5:\n",
    "            for i in range(config.num_agents):\n",
    "                torch.save(maddpg.maddpg_agents[i].actor_local.state_dict(), 'checkpoint_actor_{}_final.pth'.format(i))\n",
    "                torch.save(maddpg.maddpg_agents[i].critic_local.state_dict(), 'checkpoint_critic_{}_final.pth'.format(i))\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Max Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = ddpg_multi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the scores\n",
    "Plot the scores according to their episodes. We can see a gradual increase in the scores as we increase the training episodes.\n",
    "\n",
    "- threshold score of +0.5 in green dashed line\n",
    "- scores of agents per episode in purple\n",
    "- average maximum score per 100 episodes in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# plot scores\n",
    "t = np.arange(1,len(scores)+1)\n",
    "s = []\n",
    "i = 0\n",
    "while i < len(scores):\n",
    "    s.append(np.mean([np.max(s_ep) for s_ep in scores[i:i+100]]))\n",
    "    i += 1\n",
    "\n",
    "# plot max score/episode\n",
    "plt.plot(np.arange(1, len(scores)+1), [np.max(s_ep) for s_ep in scores])\n",
    "# plot average of max per next 100 episodes\n",
    "plt.plot(t, s, c='r', linewidth=2)\n",
    "# plot threshold line at +0.5\n",
    "plt.hlines(0.5, 0, len(scores), colors='g', linestyles='dashed')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test the trained agents\n",
    "Run a pair of **trained** agents for 1000 time steps to see what happens to the score. Compare this with the score of the untrained agents from 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.30000000447034836\n",
      "Score (max over agents) from episode 2: 0.7000000104308128\n",
      "Score (max over agents) from episode 3: 0.20000000298023224\n",
      "Score (max over agents) from episode 4: 0.30000000447034836\n",
      "Score (max over agents) from episode 5: 1.700000025331974\n"
     ]
    }
   ],
   "source": [
    "# Load policy network weights saved from training\n",
    "maddpg.maddpg_agents[0].actor_local.load_state_dict(torch.load('checkpoint_actor_0_final.pth'))\n",
    "maddpg.maddpg_agents[1].actor_local.load_state_dict(torch.load('checkpoint_actor_1_final.pth'))\n",
    "\n",
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = maddpg.act(states)                       # select an action for each agent\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
